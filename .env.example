# PromptSentinel Configuration

# API Configuration
# API_HOST: Network interface to bind to
#   - 0.0.0.0 = Listen on all interfaces (allows external connections)
#   - 127.0.0.1 or localhost = Local connections only (more secure for development)
API_HOST=0.0.0.0

# API_PORT: TCP port for the service
#   - 8080 = Common development port (no special privileges needed)
#   - 80 = Standard HTTP (requires root/admin on some systems)
#   - 443 = Standard HTTPS (requires SSL cert configuration)
API_PORT=8080

# API_ENV: Environment mode affecting security and performance
#   - development = Verbose errors, relaxed security, auto-reload
#   - staging = Production-like with some debug features
#   - production = Optimized performance, strict security, minimal logging
API_ENV=development

# DEBUG: Enable/disable debug mode
#   - true = Show stack traces, enable auto-reload, verbose logging
#   - false = Hide sensitive errors, better performance
#   ⚠️ ALWAYS set to false in production for security
DEBUG=true

# LLM Provider Configuration
# Order of providers to try (comma-separated)
LLM_PROVIDER_ORDER=anthropic,openai,gemini

# Primary: Anthropic
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL=claude-3-haiku-20240307
ANTHROPIC_MAX_TOKENS=1000
ANTHROPIC_TEMPERATURE=0.3

# Secondary: OpenAI
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4-turbo-preview
OPENAI_MAX_TOKENS=1000
OPENAI_TEMPERATURE=0.3

# Fallback: Google Gemini
GEMINI_API_KEY=your-gemini-api-key
GEMINI_MODEL=gemini-1.5-flash  # gemini-pro is deprecated, use gemini-1.5-flash or gemini-2.0-flash
GEMINI_MAX_TOKENS=1000
GEMINI_TEMPERATURE=0.3

# Redis Cache Configuration (optional - system works without it)
# Redis provides optional caching to reduce LLM API calls by 30-40%
REDIS_ENABLED=false
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=changeme-in-production
REDIS_TTL=3600

# Cache TTLs for different types (seconds)
CACHE_TTL_LLM=3600         # LLM classification results (1 hour)
CACHE_TTL_DETECTION=600    # Full detection results (10 minutes)
CACHE_TTL_PATTERN=1800     # Pattern matching results (30 minutes)
CACHE_TTL_HEALTH=60        # Health check results (1 minute)

# Detection Configuration
DETECTION_MODE=strict  # strict, moderate, permissive
DETECTION_TIMEOUT=10.0
HEURISTIC_ENABLED=true
LLM_CLASSIFICATION_ENABLED=true
CONFIDENCE_THRESHOLD=0.7

# Budget Configuration
# Set spending limits to control LLM API costs
BUDGET_HOURLY_LIMIT=10.0        # Maximum spend per hour in USD
BUDGET_DAILY_LIMIT=100.0         # Maximum spend per day in USD
BUDGET_MONTHLY_LIMIT=1000.0      # Maximum spend per month in USD
BUDGET_BLOCK_ON_EXCEEDED=true    # Block requests when budget exceeded
BUDGET_PREFER_CACHE=true         # Prefer cached results to save costs

# Rate Limiting Configuration
# Control request throughput to prevent abuse and manage load
RATE_LIMIT_REQUESTS_PER_MINUTE=60           # Global requests per minute
RATE_LIMIT_TOKENS_PER_MINUTE=10000          # Global tokens per minute
RATE_LIMIT_CLIENT_REQUESTS_PER_MINUTE=20    # Per-client requests per minute

# Security Configuration
MAX_PROMPT_LENGTH=50000
RATE_LIMIT_PER_IP=1000
RATE_LIMIT_PER_KEY=10000
ALLOWED_CHARSETS=utf-8,ascii

# Monitoring & Logging
LOG_LEVEL=INFO
LOG_FORMAT=json
ENABLE_METRICS=true
ENABLE_TRACING=false
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318

# Corpus Management
CORPUS_AUTO_UPDATE=false
CORPUS_UPDATE_INTERVAL=86400
CORPUS_SOURCES=https://example.com/corpus.jsonl

# PII Detection Configuration
PII_DETECTION_ENABLED=true
# PII Redaction modes:
#   mask: Replace PII with masked values (e.g., ***-**-1234)
#   remove: Replace with [TYPE_REMOVED] placeholders
#   hash: Replace with hashed values for consistency
#   reject: Block the entire request if PII is detected
#   pass-silent: ⚠️ DANGEROUS - Pass PII through without modification (NOT RECOMMENDED)
#   pass-alert: ⚠️ WARNING - Pass PII through but log and alert (USE WITH CAUTION)
PII_REDACTION_MODE=mask
PII_TYPES_TO_DETECT=all  # all or comma-separated: credit_card,ssn,email,phone
PII_LOG_DETECTED=false  # Set to true with pass-alert to log PII detection warnings
PII_CONFIDENCE_THRESHOLD=0.7

# ============================================================================
# PRODUCTION CONFIGURATION EXAMPLE
# ============================================================================
# For production deployments, consider these recommended settings:
#
# API_HOST=0.0.0.0                        # Or specific IP for security
# API_PORT=80                             # Standard port (or 443 with SSL)
# API_ENV=production                      # Production mode
# DEBUG=false                             # ⚠️ CRITICAL: Must be false
#
# DETECTION_MODE=strict                   # Maximum security
# LLM_CLASSIFICATION_ENABLED=true         # Enable for best accuracy
# PII_REDACTION_MODE=reject               # Zero tolerance for PII
# PII_LOG_DETECTED=false                  # Never log actual PII
#
# RATE_LIMIT_PER_IP=100                   # Stricter rate limits
# MAX_PROMPT_LENGTH=10000                 # Smaller limit to prevent abuse
#
# LOG_LEVEL=WARNING                       # Less verbose logging
# ENABLE_METRICS=true                     # Enable monitoring
# ENABLE_TRACING=true                     # Enable distributed tracing
# ============================================================================