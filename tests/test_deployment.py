"""Deployment and rollback tests for PromptSentinel."""

import pytest
import asyncio
import os
import json
import yaml
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Any
from unittest.mock import AsyncMock, MagicMock, patch
import tempfile
import shutil
from pathlib import Path

from prompt_sentinel.models.schemas import Message, Role, Verdict


class TestDeploymentPipeline:
    """Test deployment pipeline stages."""

    @pytest.fixture
    def deployment_manager(self):
        """Create deployment manager."""
        from prompt_sentinel.deployment.manager import DeploymentManager
        return DeploymentManager(
            environment="staging",
            config_path="deployment/config.yaml"
        )

    @pytest.mark.asyncio
    async def test_pre_deployment_checks(self, deployment_manager):
        """Test pre-deployment validation checks."""
        checks = await deployment_manager.run_pre_deployment_checks()
        
        assert checks["tests_passed"] is True
        assert checks["code_coverage"] >= 80
        assert checks["security_scan"]["vulnerabilities"] == 0
        assert checks["dependencies_updated"] is True
        assert checks["migrations_ready"] is True
        assert checks["config_valid"] is True

    @pytest.mark.asyncio
    async def test_deployment_stages(self, deployment_manager):
        """Test deployment through multiple stages."""
        stages = ["build", "test", "package", "deploy", "verify"]
        
        for stage in stages:
            result = await deployment_manager.execute_stage(stage)
            
            assert result["status"] == "success"
            assert result["duration"] > 0
            assert "artifacts" in result
            
            if stage == "build":
                assert "image_tag" in result["artifacts"]
            elif stage == "test":
                assert result["tests_passed"] is True
            elif stage == "deploy":
                assert "deployment_id" in result

    @pytest.mark.asyncio
    async def test_blue_green_deployment(self, deployment_manager):
        """Test blue-green deployment strategy."""
        # Deploy to green environment
        green_deployment = await deployment_manager.deploy_blue_green(
            target="green",
            version="v2.0.0"
        )
        
        assert green_deployment["environment"] == "green"
        assert green_deployment["status"] == "running"
        
        # Run smoke tests
        smoke_tests = await deployment_manager.run_smoke_tests("green")
        assert smoke_tests["passed"] is True
        
        # Switch traffic
        switch_result = await deployment_manager.switch_traffic(
            from_env="blue",
            to_env="green"
        )
        
        assert switch_result["traffic_switched"] is True
        assert switch_result["green_traffic"] == 100
        assert switch_result["blue_traffic"] == 0

    @pytest.mark.asyncio
    async def test_canary_deployment(self, deployment_manager):
        """Test canary deployment strategy."""
        # Start canary deployment
        canary = await deployment_manager.deploy_canary(
            version="v2.0.0",
            initial_traffic_percent=5
        )
        
        assert canary["status"] == "in_progress"
        assert canary["traffic_percentage"] == 5
        
        # Monitor canary metrics
        for percentage in [10, 25, 50, 100]:
            metrics = await deployment_manager.get_canary_metrics()
            
            if metrics["error_rate"] < 0.01 and metrics["latency_p99"] < 100:
                # Increase traffic
                result = await deployment_manager.increase_canary_traffic(percentage)
                assert result["traffic_percentage"] == percentage
            else:
                # Rollback
                rollback = await deployment_manager.rollback_canary()
                assert rollback["status"] == "rolled_back"
                break

    @pytest.mark.asyncio
    async def test_rolling_update(self, deployment_manager):
        """Test rolling update deployment."""
        # Configure rolling update
        config = {
            "max_surge": 2,
            "max_unavailable": 1,
            "update_batch_size": 3,
            "pause_between_batches": 30
        }
        
        deployment = await deployment_manager.rolling_update(
            version="v2.0.0",
            config=config
        )
        
        # Monitor progress
        while deployment["status"] == "in_progress":
            status = await deployment_manager.get_deployment_status(
                deployment["id"]
            )
            
            assert status["healthy_instances"] >= status["total_instances"] - 1
            assert status["updated_instances"] <= status["total_instances"]
            
            await asyncio.sleep(1)
        
        assert deployment["status"] == "completed"


class TestContainerDeployment:
    """Test container-based deployment."""

    @pytest.fixture
    def container_manager(self):
        """Create container manager."""
        from prompt_sentinel.deployment.containers import ContainerManager
        return ContainerManager()

    @pytest.mark.asyncio
    async def test_docker_build(self, container_manager):
        """Test Docker image building."""
        # Build image
        build_result = await container_manager.build_image(
            dockerfile="Dockerfile",
            context=".",
            tags=["prompt-sentinel:latest", "prompt-sentinel:v2.0.0"]
        )
        
        assert build_result["success"] is True
        assert "image_id" in build_result
        assert len(build_result["tags"]) == 2

    @pytest.mark.asyncio
    async def test_container_health_checks(self, container_manager):
        """Test container health check configuration."""
        health_check = {
            "test": ["CMD", "curl", "-f", "http://localhost:8000/health"],
            "interval": 30,
            "timeout": 10,
            "retries": 3,
            "start_period": 60
        }
        
        container = await container_manager.run_container(
            image="prompt-sentinel:latest",
            health_check=health_check
        )
        
        # Wait for health check
        health_status = await container_manager.wait_for_healthy(
            container["id"],
            timeout=120
        )
        
        assert health_status == "healthy"

    @pytest.mark.asyncio
    async def test_container_orchestration(self, container_manager):
        """Test container orchestration with Docker Compose."""
        compose_config = {
            "version": "3.8",
            "services": {
                "api": {
                    "image": "prompt-sentinel:latest",
                    "ports": ["8000:8000"],
                    "environment": {
                        "ENV": "production",
                        "LOG_LEVEL": "info"
                    },
                    "depends_on": ["redis", "postgres"]
                },
                "redis": {
                    "image": "redis:7-alpine"
                },
                "postgres": {
                    "image": "postgres:15-alpine",
                    "environment": {
                        "POSTGRES_DB": "promptsentinel",
                        "POSTGRES_PASSWORD": "secret"
                    }
                }
            }
        }
        
        # Deploy stack
        stack = await container_manager.deploy_stack(
            name="prompt-sentinel",
            config=compose_config
        )
        
        assert stack["status"] == "running"
        assert len(stack["services"]) == 3
        
        # Check service health
        for service in stack["services"]:
            health = await container_manager.check_service_health(service)
            assert health["status"] == "healthy"


class TestKubernetesDeployment:
    """Test Kubernetes deployment."""

    @pytest.fixture
    def k8s_manager(self):
        """Create Kubernetes manager."""
        from prompt_sentinel.deployment.kubernetes import KubernetesManager
        return KubernetesManager(
            namespace="prompt-sentinel",
            context="production"
        )

    @pytest.mark.asyncio
    async def test_kubernetes_deployment(self, k8s_manager):
        """Test Kubernetes deployment creation."""
        deployment_spec = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": "prompt-sentinel",
                "labels": {"app": "prompt-sentinel"}
            },
            "spec": {
                "replicas": 3,
                "selector": {
                    "matchLabels": {"app": "prompt-sentinel"}
                },
                "template": {
                    "metadata": {
                        "labels": {"app": "prompt-sentinel"}
                    },
                    "spec": {
                        "containers": [{
                            "name": "api",
                            "image": "prompt-sentinel:v2.0.0",
                            "ports": [{"containerPort": 8000}],
                            "resources": {
                                "requests": {"memory": "256Mi", "cpu": "250m"},
                                "limits": {"memory": "512Mi", "cpu": "500m"}
                            },
                            "livenessProbe": {
                                "httpGet": {"path": "/health", "port": 8000},
                                "initialDelaySeconds": 30,
                                "periodSeconds": 10
                            },
                            "readinessProbe": {
                                "httpGet": {"path": "/ready", "port": 8000},
                                "initialDelaySeconds": 10,
                                "periodSeconds": 5
                            }
                        }]
                    }
                }
            }
        }
        
        # Apply deployment
        result = await k8s_manager.apply_deployment(deployment_spec)
        
        assert result["created"] is True
        assert result["replicas_desired"] == 3
        
        # Wait for rollout
        rollout = await k8s_manager.wait_for_rollout(
            "prompt-sentinel",
            timeout=300
        )
        
        assert rollout["status"] == "completed"
        assert rollout["replicas_ready"] == 3

    @pytest.mark.asyncio
    async def test_horizontal_pod_autoscaling(self, k8s_manager):
        """Test HPA configuration."""
        hpa_spec = {
            "apiVersion": "autoscaling/v2",
            "kind": "HorizontalPodAutoscaler",
            "metadata": {"name": "prompt-sentinel-hpa"},
            "spec": {
                "scaleTargetRef": {
                    "apiVersion": "apps/v1",
                    "kind": "Deployment",
                    "name": "prompt-sentinel"
                },
                "minReplicas": 2,
                "maxReplicas": 10,
                "metrics": [
                    {
                        "type": "Resource",
                        "resource": {
                            "name": "cpu",
                            "target": {
                                "type": "Utilization",
                                "averageUtilization": 70
                            }
                        }
                    },
                    {
                        "type": "Resource",
                        "resource": {
                            "name": "memory",
                            "target": {
                                "type": "Utilization",
                                "averageUtilization": 80
                            }
                        }
                    }
                ]
            }
        }
        
        result = await k8s_manager.apply_hpa(hpa_spec)
        
        assert result["created"] is True
        assert result["min_replicas"] == 2
        assert result["max_replicas"] == 10

    @pytest.mark.asyncio
    async def test_service_mesh_deployment(self, k8s_manager):
        """Test service mesh integration."""
        # Deploy with Istio sidecar
        deployment = await k8s_manager.deploy_with_istio(
            name="prompt-sentinel",
            image="prompt-sentinel:v2.0.0",
            traffic_policy={
                "connectionPool": {
                    "tcp": {"maxConnections": 100},
                    "http": {"http1MaxPendingRequests": 50}
                },
                "outlierDetection": {
                    "consecutiveErrors": 5,
                    "interval": "30s",
                    "baseEjectionTime": "30s"
                }
            }
        )
        
        assert deployment["istio_enabled"] is True
        assert "sidecar_injected" in deployment
        assert deployment["traffic_policy_applied"] is True


class TestRollbackStrategies:
    """Test rollback strategies and procedures."""

    @pytest.fixture
    def rollback_manager(self):
        """Create rollback manager."""
        from prompt_sentinel.deployment.rollback import RollbackManager
        return RollbackManager()

    @pytest.mark.asyncio
    async def test_automatic_rollback(self, rollback_manager):
        """Test automatic rollback on failure."""
        # Configure rollback triggers
        triggers = {
            "error_rate_threshold": 0.05,
            "latency_p99_threshold": 1000,
            "health_check_failures": 3,
            "monitor_duration": 300
        }
        
        await rollback_manager.configure_triggers(triggers)
        
        # Simulate deployment with issues
        deployment = await rollback_manager.monitor_deployment(
            deployment_id="deploy_123",
            version="v2.0.0"
        )
        
        # Simulate metrics exceeding thresholds
        await rollback_manager.record_metric("error_rate", 0.08)
        
        # Should trigger rollback
        rollback = await rollback_manager.check_and_rollback()
        
        assert rollback["triggered"] is True
        assert rollback["reason"] == "error_rate exceeded threshold"
        assert rollback["target_version"] == "v1.9.0"

    @pytest.mark.asyncio
    async def test_manual_rollback(self, rollback_manager):
        """Test manual rollback process."""
        # Get rollback targets
        targets = await rollback_manager.get_rollback_targets()
        
        assert len(targets) > 0
        assert all(t["status"] == "stable" for t in targets)
        
        # Perform rollback
        rollback = await rollback_manager.rollback_to_version(
            version=targets[0]["version"],
            reason="Manual rollback for testing"
        )
        
        assert rollback["status"] == "in_progress"
        
        # Monitor rollback
        while rollback["status"] == "in_progress":
            status = await rollback_manager.get_rollback_status(rollback["id"])
            assert status["progress"] <= 100
            await asyncio.sleep(1)
        
        assert rollback["status"] == "completed"

    @pytest.mark.asyncio
    async def test_database_rollback(self, rollback_manager):
        """Test database migration rollback."""
        # Rollback database
        db_rollback = await rollback_manager.rollback_database(
            from_version="2.0.0",
            to_version="1.9.0"
        )
        
        assert db_rollback["migrations_reversed"] > 0
        assert db_rollback["data_preserved"] is True
        assert db_rollback["integrity_verified"] is True

    @pytest.mark.asyncio
    async def test_partial_rollback(self, rollback_manager):
        """Test partial/component rollback."""
        # Rollback specific component
        component_rollback = await rollback_manager.rollback_component(
            component="detection_service",
            version="v1.9.0",
            keep_others=True
        )
        
        assert component_rollback["component"] == "detection_service"
        assert component_rollback["other_components_unchanged"] is True
        assert component_rollback["compatibility_verified"] is True


class TestDeploymentVerification:
    """Test deployment verification and validation."""

    @pytest.fixture
    def verification_manager(self):
        """Create verification manager."""
        from prompt_sentinel.deployment.verification import VerificationManager
        return VerificationManager()

    @pytest.mark.asyncio
    async def test_smoke_tests(self, verification_manager):
        """Test smoke test execution."""
        smoke_tests = [
            {"name": "health_check", "endpoint": "/health"},
            {"name": "api_responsive", "endpoint": "/v1/detect", "method": "POST"},
            {"name": "database_connected", "check": "db_connection"},
            {"name": "cache_working", "check": "redis_ping"}
        ]
        
        results = await verification_manager.run_smoke_tests(smoke_tests)
        
        assert all(r["passed"] for r in results)
        assert all(r["response_time"] < 1000 for r in results)

    @pytest.mark.asyncio
    async def test_integration_tests(self, verification_manager):
        """Test post-deployment integration tests."""
        # Run integration test suite
        integration_results = await verification_manager.run_integration_tests(
            test_suite="deployment_integration",
            environment="staging"
        )
        
        assert integration_results["total_tests"] > 0
        assert integration_results["passed_tests"] == integration_results["total_tests"]
        assert integration_results["critical_paths_tested"] is True

    @pytest.mark.asyncio
    async def test_performance_validation(self, verification_manager):
        """Test performance validation after deployment."""
        # Run performance tests
        perf_results = await verification_manager.validate_performance(
            baseline={
                "latency_p50": 50,
                "latency_p99": 200,
                "throughput": 1000
            }
        )
        
        assert perf_results["latency_p50"] <= 55  # Within 10% of baseline
        assert perf_results["latency_p99"] <= 220
        assert perf_results["throughput"] >= 900

    @pytest.mark.asyncio
    async def test_security_validation(self, verification_manager):
        """Test security validation after deployment."""
        security_checks = await verification_manager.run_security_validation()
        
        assert security_checks["ssl_enabled"] is True
        assert security_checks["headers_configured"] is True
        assert security_checks["rate_limiting_active"] is True
        assert security_checks["authentication_working"] is True
        assert security_checks["vulnerable_dependencies"] == 0


class TestConfigurationManagement:
    """Test configuration management during deployment."""

    @pytest.fixture
    def config_manager(self):
        """Create configuration manager."""
        from prompt_sentinel.deployment.config import ConfigurationManager
        return ConfigurationManager()

    @pytest.mark.asyncio
    async def test_config_validation(self, config_manager):
        """Test configuration validation."""
        config = {
            "api": {
                "port": 8000,
                "host": "0.0.0.0",
                "workers": 4
            },
            "database": {
                "url": "postgresql://localhost/promptsentinel",
                "pool_size": 20
            },
            "redis": {
                "url": "redis://localhost:6379",
                "ttl": 3600
            }
        }
        
        validation = await config_manager.validate_config(config)
        
        assert validation["valid"] is True
        assert len(validation["warnings"]) == 0
        assert validation["environment_compatible"] is True

    @pytest.mark.asyncio
    async def test_secret_management(self, config_manager):
        """Test secret management during deployment."""
        # Store secrets
        secrets = {
            "api_key": "secret_key_123",
            "database_password": "db_pass_456",
            "jwt_secret": "jwt_secret_789"
        }
        
        for key, value in secrets.items():
            result = await config_manager.store_secret(key, value)
            assert result["stored"] is True
            assert result["encrypted"] is True
        
        # Retrieve secrets
        for key in secrets.keys():
            secret = await config_manager.get_secret(key)
            assert secret is not None
            assert secret != secrets[key]  # Should be encrypted

    @pytest.mark.asyncio
    async def test_environment_variables(self, config_manager):
        """Test environment variable configuration."""
        env_vars = {
            "PROMPT_SENTINEL_ENV": "production",
            "LOG_LEVEL": "info",
            "WORKERS": "4",
            "ENABLE_METRICS": "true"
        }
        
        # Set environment variables
        result = await config_manager.set_environment_variables(env_vars)
        assert result["set_count"] == len(env_vars)
        
        # Validate environment
        validation = await config_manager.validate_environment()
        assert validation["all_required_set"] is True

    @pytest.mark.asyncio
    async def test_feature_flags(self, config_manager):
        """Test feature flag management."""
        # Set feature flags
        flags = {
            "new_detection_algorithm": False,
            "enhanced_logging": True,
            "experimental_api": False
        }
        
        for flag, value in flags.items():
            await config_manager.set_feature_flag(flag, value)
        
        # Get flag values
        for flag, expected in flags.items():
            value = await config_manager.get_feature_flag(flag)
            assert value == expected


class TestDeploymentMonitoring:
    """Test deployment monitoring and alerting."""

    @pytest.fixture
    def monitoring_manager(self):
        """Create monitoring manager."""
        from prompt_sentinel.deployment.monitoring import MonitoringManager
        return MonitoringManager()

    @pytest.mark.asyncio
    async def test_deployment_metrics(self, monitoring_manager):
        """Test deployment metrics collection."""
        # Collect deployment metrics
        metrics = await monitoring_manager.collect_deployment_metrics(
            deployment_id="deploy_123"
        )
        
        assert "deployment_duration" in metrics
        assert "instances_deployed" in metrics
        assert "success_rate" in metrics
        assert "rollback_count" in metrics

    @pytest.mark.asyncio
    async def test_real_time_monitoring(self, monitoring_manager):
        """Test real-time deployment monitoring."""
        # Start monitoring
        monitor_id = await monitoring_manager.start_monitoring(
            deployment_id="deploy_123",
            interval=5
        )
        
        # Get real-time updates
        for _ in range(3):
            update = await monitoring_manager.get_monitoring_update(monitor_id)
            
            assert "timestamp" in update
            assert "health_status" in update
            assert "error_rate" in update
            assert "response_time" in update
            
            await asyncio.sleep(5)
        
        # Stop monitoring
        await monitoring_manager.stop_monitoring(monitor_id)

    @pytest.mark.asyncio
    async def test_alerting_during_deployment(self, monitoring_manager):
        """Test alerting during deployment issues."""
        # Configure alerts
        alert_config = {
            "channels": ["email", "slack"],
            "conditions": {
                "deployment_failed": "critical",
                "high_error_rate": "warning",
                "slow_rollout": "info"
            }
        }
        
        await monitoring_manager.configure_alerts(alert_config)
        
        # Simulate deployment issue
        await monitoring_manager.trigger_condition("high_error_rate", {
            "error_rate": 0.08,
            "threshold": 0.05
        })
        
        # Check alerts sent
        alerts = await monitoring_manager.get_sent_alerts()
        
        assert len(alerts) > 0
        assert alerts[0]["severity"] == "warning"
        assert "slack" in alerts[0]["channels"]


class TestInfrastructureAsCode:
    """Test Infrastructure as Code deployment."""

    @pytest.fixture
    def iac_manager(self):
        """Create IaC manager."""
        from prompt_sentinel.deployment.iac import IaCManager
        return IaCManager()

    @pytest.mark.asyncio
    async def test_terraform_deployment(self, iac_manager):
        """Test Terraform-based deployment."""
        terraform_config = """
        resource "aws_ecs_service" "prompt_sentinel" {
          name            = "prompt-sentinel"
          cluster         = aws_ecs_cluster.main.id
          task_definition = aws_ecs_task_definition.app.arn
          desired_count   = 3
          
          deployment_configuration {
            maximum_percent         = 200
            minimum_healthy_percent = 100
          }
        }
        """
        
        # Plan deployment
        plan = await iac_manager.terraform_plan(terraform_config)
        
        assert plan["changes_detected"] is True
        assert plan["resources_to_create"] > 0
        assert plan["estimated_cost"] is not None
        
        # Apply deployment
        result = await iac_manager.terraform_apply(plan["plan_id"])
        
        assert result["status"] == "applied"
        assert result["resources_created"] == plan["resources_to_create"]

    @pytest.mark.asyncio
    async def test_ansible_configuration(self, iac_manager):
        """Test Ansible-based configuration."""
        playbook = """
        - hosts: prompt_sentinel
          tasks:
            - name: Update application
              docker_container:
                name: prompt_sentinel
                image: prompt-sentinel:v2.0.0
                state: started
                restart: yes
        """
        
        # Run playbook
        result = await iac_manager.run_ansible_playbook(
            playbook=playbook,
            inventory="production"
        )
        
        assert result["successful_hosts"] > 0
        assert result["failed_hosts"] == 0
        assert result["tasks_completed"] > 0

    @pytest.mark.asyncio
    async def test_helm_chart_deployment(self, iac_manager):
        """Test Helm chart deployment."""
        # Deploy using Helm
        deployment = await iac_manager.helm_install(
            chart="prompt-sentinel",
            release="prompt-sentinel-prod",
            namespace="production",
            values={
                "replicaCount": 3,
                "image": {
                    "repository": "prompt-sentinel",
                    "tag": "v2.0.0"
                },
                "resources": {
                    "limits": {"cpu": "500m", "memory": "512Mi"},
                    "requests": {"cpu": "250m", "memory": "256Mi"}
                }
            }
        )
        
        assert deployment["status"] == "deployed"
        assert deployment["revision"] == 1


if __name__ == "__main__":
    pytest.main([__file__, "-v"])